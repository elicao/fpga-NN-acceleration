{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cProfile\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import truncnorm\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28 # width and length\n",
    "no_of_different_labels = 10 #  i.e. 0, 1, 2, 3, ..., 9\n",
    "image_pixels = image_size * image_size\n",
    "data_path = \"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(data_path + \"mnist_test.1k.csv\", delimiter=\",\").values\n",
    "\n",
    "fac = 0.99 / 255\n",
    "test_imgs = np.asfarray(test_data[:, 1:], dtype=np.float32) * fac + 0.01\n",
    "test_imgs = test_imgs.reshape(test_imgs.shape[0], 1, test_imgs.shape[1])\n",
    "\n",
    "test_labels = np.asfarray(test_data[:, :1], dtype=np.float32)\n",
    "\n",
    "lr = np.arange(no_of_different_labels)\n",
    "# transform labels into one hot representation\n",
    "test_labels_one_hot = (lr==test_labels).astype(np.float32)\n",
    "\n",
    "# we don't want zeroes and ones in the labels neither:\n",
    "test_labels_one_hot[test_labels_one_hot==0] = 0.001\n",
    "test_labels_one_hot[test_labels_one_hot==1] = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # dBias = output_error\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhLayer(ActivationLayer):\n",
    "    # static\n",
    "    e = 2.71828182845904523536028747135266249775724709369995\n",
    "    \n",
    "    #http://www.plunk.org/~hatch/rightway.php\n",
    "    #https://math.stackexchange.com/questions/518758/alternative-form-for-sinhx-coshx\n",
    "    @staticmethod\n",
    "    def tanh(x):   \n",
    "        e = TanhLayer.e\n",
    "        return (1 - e ** (-2 * x)) / (1 + e ** (-2 * x)) \n",
    "        #return (1-np.exp(-2 * x))/(1+np.exp(-2 * x))\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh_prime(x):\n",
    "        return 1-TanhLayer.tanh(x)**2\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TanhLayer,self).__init__(self.tanh, self.tanh_prime)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
    "    \n",
    "    def save(self, fname):\n",
    "        import pickle\n",
    "        with open(fname, \"bw\") as fh:\n",
    "            pickle.dump(self, fh)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, fname):\n",
    "        import pickle\n",
    "        with open(fname, \"br\") as fh:\n",
    "            return pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network.load('network.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "Layer Name: FCLayer\n",
      "Layer is FC: True\n",
      "Weights:  (784, 80)\n",
      "\n",
      "\n",
      "Index: 2\n",
      "Layer Name: FCLayer\n",
      "Layer is FC: True\n",
      "Weights:  (80, 40)\n",
      "\n",
      "\n",
      "Index: 4\n",
      "Layer Name: FCLayer\n",
      "Layer is FC: True\n",
      "Weights:  (40, 20)\n",
      "\n",
      "\n",
      "Index: 6\n",
      "Layer Name: FCLayer\n",
      "Layer is FC: True\n",
      "Weights:  (20, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(net.layers)):\n",
    "    l = net.layers[i]\n",
    "    is_fc = isinstance(l, FCLayer)    \n",
    "    if is_fc:\n",
    "        print (\"Index: \" + str(i))\n",
    "        print (\"Layer Name: \" + str(l.__class__.__name__))\n",
    "        print (\"Layer is FC: \" + str(is_fc))\n",
    "        weights = l.weights\n",
    "        print (\"Weights: \", weights.shape)\n",
    "\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:  (20, 10)\n",
      "Bias:  (1, 10)\n",
      "Inputs:  (1, 20)\n",
      "Outputs:  (1, 10)\n",
      "\n",
      "Weights:  (20, 10)\n",
      "Bias:  (10,)\n",
      "Inputs:  (20,)\n",
      "Outputs:  (10,)\n"
     ]
    }
   ],
   "source": [
    "layer_id = 6\n",
    "layer = net.layers[layer_id]\n",
    "weights = layer.weights\n",
    "bias = layer.bias\n",
    "inputs = layer.input\n",
    "outputs = layer.output\n",
    "print (\"Weights: \", weights.shape)\n",
    "print (\"Bias: \", bias.shape)\n",
    "print (\"Inputs: \", inputs.shape)\n",
    "print (\"Outputs: \", outputs.shape)\n",
    "print ()\n",
    "\n",
    "# flatten the bias, input and outputs matricies into arrays\n",
    "bias = bias.flatten()\n",
    "inputs = inputs.flatten()\n",
    "outputs = outputs.flatten()\n",
    "\n",
    "print (\"Weights: \", weights.shape)\n",
    "print (\"Bias: \", bias.shape)\n",
    "print (\"Inputs: \", inputs.shape)\n",
    "print (\"Outputs: \", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.41362254e-03  2.98508890e-03 -2.72925311e-03  1.12005720e-03\n",
      "  1.34415123e-04  5.53221731e-03  1.00080436e+00 -1.55906530e-03\n",
      " -1.35419525e-03 -1.24714277e-04]\n",
      "[ 4.41370127e-03  2.98510492e-03 -2.72925683e-03  1.12005489e-03\n",
      "  1.34415926e-04  5.53221199e-03  1.00080440e+00 -1.55908733e-03\n",
      " -1.35418249e-03 -1.24661481e-04]\n"
     ]
    }
   ],
   "source": [
    "# how its done in dot.sv\n",
    "def dot(weights,inputs):\n",
    "    outs = np.zeros(weights.shape[1], dtype=np.float32)\n",
    "    for i in range(weights.shape[0]): # input length\n",
    "        for j in range(weights.shape[1]): # output length\n",
    "            outs[j] = outs[j] + weights[i][j] * inputs[i]\n",
    "    return outs\n",
    "\n",
    "# my results\n",
    "print (dot(weights,inputs) + bias)\n",
    "# reference results\n",
    "print (outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create some Verilog arrays\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "def flt_to_hex(f):\n",
    "    return hex(struct.unpack('<I', struct.pack('<f', f))[0])\n",
    "\n",
    "file_name = \"sim_last_layer.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// See sim_last_layer.ipynb for more details\n",
      "localparam ROWS = 20;\n",
      "localparam COLS = 10;\n",
      "\n",
      "//weights:\n",
      "//[[ 2.58326735e-02  3.10679128e-02  7.49541483e-02  6.94312717e-02\n",
      "//   2.79411147e-02 -3.88900762e-01  3.34132064e-02  2.34185058e-02\n",
      "//   6.41438352e-02  3.90812610e-02]\n",
      "// [ 4.74299141e-01 -3.07274602e-02 -7.55915272e-02 -6.56207528e-02\n",
      "//  -2.70176780e-02 -1.19249905e-01 -3.36554767e-02 -2.31518535e-02\n",
      "//  -6.25688793e-02 -3.85192568e-02]\n",
      "// [ 1.10262082e-01 -2.19398860e-01  1.35315516e-01  3.27390767e-02\n",
      "//  -1.00836485e-01  3.15356007e-02 -7.07275928e-02 -6.07912470e-02\n",
      "//   1.09173516e-01 -9.76181188e-02]\n",
      "// [-9.52519917e-02 -1.53209934e-02  2.77056965e-01  2.98086684e-01\n",
      "//  -7.57019013e-02 -3.56717738e-01 -3.63614657e-01 -1.97010157e-01\n",
      "//   7.03833589e-02 -6.57219942e-02]\n",
      "// [-5.40141227e-03 -1.75347015e-03 -2.18014605e-02 -4.35117789e-02\n",
      "//   1.26061503e-02 -6.80273724e-02 -2.23201639e-02  5.77344579e-02\n",
      "//   7.78135215e-04  9.30981529e-02]\n",
      "// [ 1.55772139e-01  3.44770828e-01  9.01376770e-02 -3.13243040e-01\n",
      "//  -3.66553196e-01  2.01121416e-02 -1.75364886e-01  3.09369223e-01\n",
      "//  -3.30585459e-01 -1.45996120e-01]\n",
      "// [ 8.33356958e-02  3.54639690e-01  3.44559428e-02 -1.74484118e-01\n",
      "//  -1.44074337e-01  3.40118569e-01 -3.73859590e-01  2.16255708e-01\n",
      "//  -3.11039945e-01  4.53230623e-02]\n",
      "// [-2.59651389e-02 -3.41397624e-02 -7.42814214e-02 -6.53747888e-02\n",
      "//  -2.70026663e-02 -1.20307527e-01 -3.27941612e-02 -2.26867926e-02\n",
      "//   4.43037993e-01 -4.10003586e-02]\n",
      "// [-2.02475355e-02 -2.83679750e-02 -5.54701189e-02 -2.29975296e-02\n",
      "//   4.64298188e-01 -4.95942218e-02 -1.24055411e-02 -8.19211410e-02\n",
      "//  -6.36037437e-02 -1.31362347e-01]\n",
      "// [-8.96531306e-02 -1.56383474e-02  3.11655764e-01 -1.75561279e-03\n",
      "//   2.21237835e-01 -2.94986870e-01  1.28928266e-01 -5.99053072e-02\n",
      "//  -2.60681234e-01 -1.44203104e-01]\n",
      "// [-2.17100417e-02 -2.97184180e-02 -5.67778598e-02 -2.47681976e-02\n",
      "//  -3.73835150e-02 -5.22429431e-02 -1.05619236e-02  4.24263273e-01\n",
      "//  -6.23486253e-02 -1.29693133e-01]\n",
      "// [-3.38301948e-01 -2.13034331e-01 -4.89511165e-02  2.54409674e-01\n",
      "//  -2.25576308e-01 -9.24214229e-02 -5.50131551e-02  3.78463433e-01\n",
      "//   2.12004500e-01 -2.06328903e-01]\n",
      "// [-1.49021327e-03 -5.03972735e-01  5.08413984e-01 -1.19680895e-03\n",
      "//   4.36277347e-04 -5.92609269e-04 -1.22715626e-03 -7.26085353e-04\n",
      "//  -4.16693995e-04  2.05243473e-05]\n",
      "// [-2.64486161e-02 -3.10832985e-02 -7.60636622e-02  4.36739034e-01\n",
      "//  -2.65141620e-02 -1.17527762e-01 -3.21705933e-02 -2.22100775e-02\n",
      "//  -6.50643353e-02 -3.91149171e-02]\n",
      "// [ 3.53772813e-01 -3.77820401e-01  2.96223226e-01  2.39788292e-02\n",
      "//  -3.82095903e-01 -2.23997001e-01 -1.64205367e-01  1.00983582e-01\n",
      "//  -5.68164950e-02 -1.11620763e-01]\n",
      "// [ 2.05789360e-02  2.89201826e-02  5.40363179e-02  2.32146927e-02\n",
      "//   4.14366884e-02  4.95478651e-02  1.04765504e-02  8.13296001e-02\n",
      "//   6.43887993e-02 -3.73655504e-01]\n",
      "// [-2.66278407e-02 -3.03465702e-02 -7.81743961e-02 -6.62931526e-02\n",
      "//  -2.78230257e-02 -1.16954549e-01  4.68987733e-01 -2.16024032e-02\n",
      "//  -6.39212155e-02 -3.81482741e-02]\n",
      "// [-2.13102967e-01  2.00328013e-01 -2.06085612e-01 -2.52612068e-01\n",
      "//  -1.38192439e-01  1.94000405e-02  1.93761389e-02  2.77061303e-01\n",
      "//  -3.35038413e-01 -1.39164866e-01]\n",
      "// [ 3.81982248e-01 -2.43662847e-01  1.20944667e-01  1.89867807e-02\n",
      "//   2.05546958e-01  4.04811250e-02  2.36271681e-01 -2.86447632e-01\n",
      "//   2.28577123e-01  4.49444197e-01]\n",
      "// [-2.82219876e-02 -3.33509140e-02  4.30560828e-01 -6.82989108e-02\n",
      "//  -2.64676497e-02 -1.17585676e-01 -3.33398281e-02 -2.21418506e-02\n",
      "//  -6.44546823e-02 -3.74625435e-02]]\n",
      "localparam [31:0] weights [0:ROWS-1] [0:COLS-1] = '{\n",
      "\t'{32'h3cd39f0b,32'h3cfe8223,32'h3d99818f,32'h3d8e31fc,32'h3ce4e4c4,32'hbec71e00,32'h3d08dc49,32'h3cbfd82b,32'h3d835dd8,32'h3d2013ac},\n",
      "\t'{32'h3ef2d756,32'hbcfbb828,32'hbd9acfbb,32'hbd86642c,32'hbcdd542d,32'hbdf4394b,32'hbd09da53,32'hbcbda8f5,32'hbd80241d,32'hbd1dc65e},\n",
      "\t'{32'h3de1d116,32'hbe60aa18,32'h3e0a9027,32'h3d061969,32'hbdce835c,32'h3d012b79,32'hbd90d9a1,32'hbd79003e,32'h3ddf965d,32'hbdc7ec02},\n",
      "\t'{32'hbdc3137a,32'hbc7b04e7,32'h3e8dda69,32'h3e989ed1,32'hbd9b0999,32'hbeb6a3b5,32'hbeba2bb3,32'hbe49bd08,32'h3d902527,32'hbd869941},\n",
      "\t'{32'hbbb0fe55,32'hbae5d4b2,32'hbcb298fa,32'hbd323968,32'h3c4e8a07,32'hbd8b51ef,32'hbcb6d8c7,32'h3d6c7af8,32'h3a4bfbc5,32'h3dbeaa3f},\n",
      "\t'{32'h3e1f82bb,32'h3eb085cd,32'h3db89a1a,32'hbea06164,32'hbebbacdc,32'h3ca4c238,32'hbe3392da,32'h3e9e65a5,32'hbea9427f,32'hbe158002},\n",
      "\t'{32'h3daaabe8,32'h3eb59355,32'h3d0d21ad,32'hbe32abf7,32'hbe138839,32'h3eae2405,32'hbebf6a86,32'h3e5d7223,32'hbe9f40a1,32'h3d39a4ad},\n",
      "\t'{32'hbcd4b4d8,32'hbd0bd623,32'hbd9820dc,32'hbd85e338,32'hbcdd34b2,32'hbdf663cb,32'hbd06532c,32'hbcb9d9a7,32'h3ee2d5e0,32'hbd27effe},\n",
      "\t'{32'hbca5de29,32'hbce863f5,32'hbd6334a3,32'hbcbc6551,32'h3eedb87e,32'hbd4b2350,32'hbc4b409c,32'hbda7c645,32'hbd8242ae,32'hbe0683da},\n",
      "\t'{32'hbdb79c0f,32'hbc801bfe,32'h3e9f9158,32'hbae61c97,32'h3e628c2c,32'hbe970885,32'h3e0405c5,32'hbd755f44,32'hbe857803,32'hbe13a9fb},\n",
      "\t'{32'hbcb1d942,32'hbcf3740a,32'hbd688fe7,32'hbccae6ad,32'hbd191f75,32'hbd55fcb2,32'hbc2d0beb,32'h3ed93909,32'hbd7f6146,32'hbe04ce47},\n",
      "\t'{32'hbead35ea,32'hbe5a25ac,32'hbd4880f7,32'h3e8241fc,32'hbe66fd7a,32'hbdbd4771,32'hbd615579,32'h3ec1c5f6,32'h3e5917b5,32'hbe5347e2},\n",
      "\t'{32'hbac35342,32'hbf01045b,32'h3f02276b,32'hba9cde3f,32'h39e4bc28,32'hba1b5956,32'hbaa0d888,32'hba3e56c3,32'hb9da77b9,32'h37ac2bb3},\n",
      "\t'{32'hbcd8aac5,32'hbcfea267,32'hbd9bc744,32'h3edf9c42,32'hbcd9343a,32'hbdf0b265,32'hbd03c550,32'hbcb5f1e9,32'hbd854073,32'hbd2036f7},\n",
      "\t'{32'h3eb521b6,32'hbec171ad,32'h3e97aa92,32'h3cc46f40,32'hbec3a213,32'hbe655f78,32'hbe282574,32'h3dced07b,32'hbd68b86a,32'hbde4996d},\n",
      "\t'{32'h3ca89528,32'h3cecea05,32'h3d5d5530,32'h3cbe2cbd,32'h3d29b984,32'h3d4af2b4,32'h3c2ba5d6,32'h3da69022,32'h3d83de46,32'hbebf4fc6},\n",
      "\t'{32'hbcda22a1,32'hbcf8995f,32'hbda019e6,32'hbd87c4b4,32'hbce3ed1d,32'hbdef85de,32'h3ef01f29,32'hbcb0f786,32'hbd82e920,32'hbd1c415d},\n",
      "\t'{32'hbe5a37aa,32'h3e4d22c9,32'hbe53081b,32'hbe81565e,32'hbe0d8252,32'h3c9eecd5,32'h3c9ebab5,32'h3e8ddafb,32'hbeab8a28,32'hbe0e813c},\n",
      "\t'{32'h3ec3932d,32'hbe7982c1,32'h3df7b1d6,32'h3c9b8a2a,32'h3e527ae7,32'h3d25cf89,32'h3e71f134,32'hbe92a944,32'h3e6a101f,32'h3ee61d8d},\n",
      "\t'{32'hbce731cc,32'hbd089af8,32'h3edc7278,32'hbd8be04d,32'hbcd8d2af,32'hbdf0d0c2,32'hbd088f58,32'hbcb562d4,32'hbd8400d1,32'hbd197253}\n",
      "};\n"
     ]
    }
   ],
   "source": [
    "# Now let's the weight matrix to Verilog \n",
    "print ('// See %s for more details' % file_name)\n",
    "print ('localparam ROWS = %d;' %weights.shape[0]) # Input Size\n",
    "print ('localparam COLS = %d;' %weights.shape[1]) # Output Size\n",
    "print ()\n",
    "print ('//weights:\\n//' + str(weights).replace('\\n', '\\n//'))\n",
    "\n",
    "print ('localparam [31:0] weights [0:ROWS-1] [0:COLS-1] = \\'{')\n",
    "for i in range(weights.shape[0]):\n",
    "    flts_hex = list(map(lambda x: '32\\'h' + flt_to_hex(x)[2:], weights[i]))\n",
    "    print ('\\t\\'{' + ','.join(flts_hex)  + '}', end='')\n",
    "    print (',' if i < weights.shape[0]-1 else '')\n",
    "print ('};')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// See sim_last_layer.ipynb for more details\n",
      "parameter NUM_VALUES = 10,\n",
      "parameter [31:0] VALUES [0:NUM_VALUES-1] = {\n",
      "//+0.399316686, +0.458808510, +0.334676876, +0.464973744, +0.324586756, -0.288951742\n",
      "  32'h3ecc733d, 32'h3eeae8f3, 32'h3eab5ac4, 32'h3eee110a, 32'h3ea6303c, 32'hbe93f17c,\n",
      "//-0.214477124, +0.149038328, +0.169016160, +0.133995899\n",
      "  32'hbe5b9fe4, 32'h3e189d81, 32'h3e2d1292, 32'h3e093639 \n",
      "};\n"
     ]
    }
   ],
   "source": [
    "# bias vector\n",
    "flts = bias\n",
    "flts_hex = list(map(lambda x: '32\\'h' + flt_to_hex(x)[2:], flts))\n",
    "flts_str = list(map('{:+2.9f}'.format, flts))\n",
    "offset=6\n",
    "print ('// See %s for more details' % file_name)\n",
    "print ('parameter NUM_VALUES = %d;' % len(flts))\n",
    "print ('parameter [31:0] VALUES [0:NUM_VALUES-1] = {')\n",
    "for i in range(0, len(flts), offset):\n",
    "    print ('//' + ', '.join(flts_str[i:i+offset]), end='\\n')\n",
    "    print ('  ' + ', '.join(flts_hex[i:i+offset] ), end='')\n",
    "    print (',' if i < len(flts) - offset else ' ')\n",
    "print ('};') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static bit [31:0] fpHex [0:19] = {\n",
      " 32'h3f7fcc9b, 32'hbf7fce23, 32'hbf7f2897, 32'hbf7fb914, 32'hbf7fcae6, 32'hbf7fcd23,\n",
      " 32'hbf7ff7cb, 32'hbf7fffe4, 32'hbf7ffed8, 32'hbf7fd52c, 32'hbf7fffef, 32'hbf7fd8e1,\n",
      " 32'h3f7ffb91, 32'hbf7ffd7b, 32'h3f7f86a0, 32'h3f7feeb1, 32'h3f7f27b7, 32'h3f7f52d6,\n",
      " 32'hbf7fc58d, 32'hbf7e119e \n",
      "};\n",
      "static string fpAscii [0:19] = {\n",
      " \"0.9992\", \"-0.9992\", \"-0.9967\", \"-0.9989\", \"-0.9992\", \"-0.9992\",\n",
      " \"-0.9999\", \"-1.0000\", \"-1.0000\", \"-0.9993\", \"-1.0000\", \"-0.9994\",\n",
      " \"0.9999\", \"-1.0000\", \"0.9981\", \"0.9997\", \"0.9967\", \"0.9974\",\n",
      " \"-0.9991\", \"-0.9925\" \n",
      "};\n",
      "static int MAX_SIZE = 20;\n"
     ]
    }
   ],
   "source": [
    "# input values \n",
    "flts = inputs\n",
    "flts_hex = list(map(lambda x: '32\\'h' + flt_to_hex(x)[2:], flts))\n",
    "flts_str = list(map('\"{:2.4f}\"'.format, flts))\n",
    "offset=6\n",
    "print ('static bit [31:0] fpHex [0:' + str(len(flts)-1) + '] = {')\n",
    "for i in range(0, len(flts), offset):\n",
    "    print (' ' + ', '.join(flts_hex[i:i+offset] ), end='')\n",
    "    print (',' if i < len(flts) - offset else ' ')\n",
    "print ('};') \n",
    "print ('static string fpAscii [0:' + str(len(flts)-1) + '] = {')\n",
    "for i in range(0, len(flts), offset):\n",
    "    print (' ' + ', '.join(flts_str[i:i+offset]), end='')\n",
    "    print (',' if i < len(flts) - offset else ' ')\n",
    "print ('};')\n",
    "print ('static int MAX_SIZE = %d;' % len(flts)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static bit [31:0] fpHex [0:9] = {\n",
      " 32'h3cffeaad, 32'h3cffeaad, 32'hbcffeaad, 32'h3cffeaad, 32'h3cffeaad, 32'h3cffeaad,\n",
      " 32'h3f463fae, 32'hbcffeaad, 32'hbcffeaad, 32'hbcffeaad \n",
      "};\n",
      "static string fpAscii [0:9] = {\n",
      " \"0.0312\", \"0.0312\", \"-0.0312\", \"0.0312\", \"0.0312\", \"0.0312\",\n",
      " \"0.7744\", \"-0.0312\", \"-0.0312\", \"-0.0312\" \n",
      "};\n",
      "static int MAX_SIZE = 10;\n"
     ]
    }
   ],
   "source": [
    "# output buffer\n",
    "\n",
    "LUT_LIM = 2 # seems to be flattened\n",
    "LUT_SIZE = 64 # 64 yields 95.6% accuracy\n",
    "LUT_STEP = ((LUT_LIM+LUT_LIM)/LUT_SIZE)\n",
    "LUT_XS = ( np.arange(-LUT_LIM, LUT_LIM, step=LUT_STEP) + \n",
    "            np.arange(LUT_LIM, -LUT_LIM, step=-LUT_STEP)[::-1] ) /2\n",
    "LUT_YS = np.array( list((map( lambda x: \n",
    "                            np.tanh(x), \n",
    "                            LUT_XS))), dtype=np.float32)\n",
    "X_SHIFT = int(LUT_LIM * (2<<15))\n",
    "X_MAX = int( LUT_XS[-1] * (2<<15)) + X_SHIFT\n",
    "def vlog_tanh(x):\n",
    "    x_int = int(x * (2<<15)) & (2**32-1)    \n",
    "    x_scaled = (x_int + X_SHIFT) & (2**32-1)\n",
    "    lut_idx = x_scaled >> 12    \n",
    "    if (lut_idx & X_MAX): return LUT_YS[0]\n",
    "    elif (lut_idx >= LUT_SIZE): return LUT_YS[-1]\n",
    "    else:  return LUT_YS[lut_idx] \n",
    "\n",
    "dots = dot(weights,inputs)\n",
    "b_dots = dots + bias \n",
    "outs = list(map(vlog_tanh, b_dots))\n",
    "\n",
    "flts = outs\n",
    "flts_hex = list(map(lambda x: '32\\'h' + flt_to_hex(x)[2:], flts))\n",
    "flts_str = list(map('\"{:2.4f}\"'.format, flts))\n",
    "offset=6\n",
    "print ('static bit [31:0] fpHex [0:' + str(len(flts)-1) + '] = {')\n",
    "for i in range(0, len(flts), offset):\n",
    "    print (' ' + ', '.join(flts_hex[i:i+offset] ), end='')\n",
    "    print (',' if i < len(flts) - offset else ' ')\n",
    "print ('};') \n",
    "print ('static string fpAscii [0:' + str(len(flts)-1) + '] = {')\n",
    "for i in range(0, len(flts), offset):\n",
    "    print (' ' + ', '.join(flts_str[i:i+offset]), end='')\n",
    "    print (',' if i < len(flts) - offset else ' ')\n",
    "print ('};') \n",
    "print ('static int MAX_SIZE = %d;' % len(flts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
